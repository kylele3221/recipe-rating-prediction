{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Title Here\n",
    "\n",
    "**Name(s)**: Andrew Kim, Kyle Le\n",
    "\n",
    "**Website Link**: (your website link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "from dsc80_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data\n",
    "\n",
    "There are two data files, RAW_recipes.csv and interactions.csv, that will be referred to as the recipes and interactions data sets respectively.  The both data sets are scraped off of food.com and hold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "recipes = pd.read_csv(\"RAW_recipes.csv\")\n",
    "ratings = pd.read_csv(\"interactions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to convert the nutrition column from a string into a list and apply it\n",
    "def str_to_list(s):\n",
    "    s = s.strip()[1:-1]\n",
    "    if not s:\n",
    "        return []\n",
    "    return [float(item.strip()) for item in s.split(\",\")]\n",
    "\n",
    "recipes[\"nutrition\"] = recipes[\"nutrition\"].apply(str_to_list)\n",
    "\n",
    "# Split the nutrition column into each of its nutritional components\n",
    "nutrition_columns = [\n",
    "    \"calories\",\n",
    "    \"total fat\",\n",
    "    \"sugar\",\n",
    "    \"sodium\",\n",
    "    \"protein\",\n",
    "    \"saturated fat\",\n",
    "    \"carbohydrates\"\n",
    "]\n",
    "temp_df = pd.DataFrame(recipes[\"nutrition\"].to_list(), columns=nutrition_columns)\n",
    "recipes = recipes.drop(columns=[\"nutrition\"])\n",
    "recipes = pd.concat([recipes, temp_df], axis=1)\n",
    "\n",
    "# Merge data frames\n",
    "merged_df = recipes.merge(ratings, left_on='id', right_on='recipe_id', how='left')\n",
    "df = merged_df\n",
    "\n",
    "# Drop unncessary columns\n",
    "df = df.drop(columns=['name', 'minutes', 'contributor_id', 'submitted', 'tags',\n",
    "       'n_steps', 'steps', 'description', 'ingredients', 'n_ingredients',\n",
    "       'user_id', 'recipe_id', 'date', 'review'])\n",
    "\n",
    "\n",
    "# Get the average rating for each recipe\n",
    "df = df.groupby('id').agg({\n",
    "    'calories': 'first',\n",
    "    'total fat': 'first',\n",
    "    'sugar': 'first',\n",
    "    'sodium': 'first',\n",
    "    'protein': 'first',\n",
    "    'saturated fat': 'first',\n",
    "    'carbohydrates': 'first',\n",
    "    'rating': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "df = df.rename(columns={'rating': 'average_rating'})\n",
    "\n",
    "# 0 ratings to np.nan\n",
    "df['average_rating'] = df['average_rating'].replace(0, np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "\n",
    "This section presents a univariate analysis to explore the relationship between product ratings and various nutrition facts, such as calories, fat, and protein content. The code summarizes the distribution of ratings across different levels of each nutritional variable, often visualizing these patterns with plots or summary statistics. By examining these relationships, we aim to identify any trends or associations, such as whether items with certain nutritional profiles tend to receive higher or lower ratings. This analysis helps to uncover potential factors that may influence how products are rated by consumers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis for each rating-shows counts of eaching rating\n",
    "rating_counts = df['average_rating'].value_counts().sort_index().reset_index()\n",
    "rating_counts.columns = ['average_rating', 'count']\n",
    "\n",
    "# Create a bar chart\n",
    "fig = px.bar(\n",
    "    rating_counts,\n",
    "    x='average_rating', y='count',\n",
    "    labels={'average_rating': 'Average Rating', 'count': 'Number of Ratings'},\n",
    "    title='Distribution of Recipe Ratings'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Univariate analysis for each nutritional component in histograms-shows counts of each component\n",
    "nutrition_columns = [\n",
    "    \"calories\", \"total fat\", \"sugar\", \"sodium\",\n",
    "    \"protein\", \"saturated fat\", \"carbohydrates\"\n",
    "]\n",
    "\n",
    "# Set the maximum value for each nutritional component for the x-axis\n",
    "x_max = {\n",
    "    \"calories\": 2500,\n",
    "    \"total fat\": 600,\n",
    "    \"sugar\": 2000,\n",
    "    \"sodium\": 2000,\n",
    "    \"protein\": 300,\n",
    "    \"saturated fat\": 500,\n",
    "    \"carbohydrates\": 300\n",
    "}\n",
    "\n",
    "# Create histograms for each nutritional component\n",
    "for col in nutrition_columns:\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=col,\n",
    "        nbins=50,\n",
    "        title=f'Distribution of {col.title()}',\n",
    "        labels={col: col.title()},\n",
    "\n",
    "        # To help with showing most of the data, hides outliers\n",
    "        range_x=[0, x_max[col]]\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    "\n",
    "In this section, we conduct a bivariate analysis to investigate how product ratings relate to specific nutrition facts, such as calories, fat, or protein. The code examines the association between ratings and each nutritional variable by comparing the distribution of ratings across different levels or categories of the nutrition fact. Visualization techniques (such as scatter plots or boxplots) and summary statistics are used to explore whether there is a systematic relationship—such as higher ratings for lower-calorie items or vice versa. This analysis provides deeper insight into how multiple variables interact and can reveal potential patterns that might not be evident in univariate analyses.  Plotting all of the data points caused extreme lag so a random set of 5000 data points were chosen to represent the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = [\n",
    "    \"calories\",\n",
    "    \"total fat\",\n",
    "    \"sugar\",\n",
    "    \"sodium\",\n",
    "    \"protein\",\n",
    "    \"saturated fat\",\n",
    "    \"carbohydrates\"\n",
    "]\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df_sample = df.sample(n=5000, random_state=42) if len(df) > 5000 else df.copy()\n",
    "\n",
    "# Using graphs to visualize relationships\n",
    "for nutrient in nutrients:\n",
    "    # Plot the downsampled data\n",
    "    plt.scatter(df_sample['average_rating'], df_sample[nutrient],  alpha=0.2)\n",
    "    plt.xlabel('Rating')\n",
    "    plt.ylabel('Calories')\n",
    "    plt.title('Scatter plot of Calories vs Rating (Downsampled)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting Aggregates\n",
    "\n",
    "This section explores interesting aggregate statistics derived from the dataset, focusing on summary measures that reveal broader patterns or notable outliers. The code calculates group-level metrics, such as the average rating within categories defined by nutrition facts (e.g., highest-rated low-calorie products or products with the most protein). By aggregating the data in different ways, we can identify standout items, observe trends across groups, and highlight key insights that might be obscured at the individual level. These aggregates help to summarize the data and point to potential areas for further investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates bins of uniform width (500)\n",
    "bin_width = 500\n",
    "max_cut = 2500\n",
    "\n",
    "cal_edges = np.arange(0, max_cut + bin_width, bin_width)\n",
    "\n",
    "# Cuts off any values above 2500\n",
    "df[\"calories_clipped\"] = df[\"calories\"].clip(upper=max_cut)\n",
    "\n",
    "# Equal bins\n",
    "df[\"calories_bin\"] = pd.cut(\n",
    "    df[\"calories_clipped\"],\n",
    "    bins=cal_edges,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Compute mean rating per bin\n",
    "agg_cal = (\n",
    "    df\n",
    "    .groupby(\"calories_bin\", observed=True)[\"average_rating\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "agg_cal[\"bin_str\"] = agg_cal[\"calories_bin\"] \\\n",
    "    .apply(lambda iv: f\"{int(iv.left)}–{int(iv.right)}\")\n",
    "\n",
    "fig = px.bar(\n",
    "    agg_cal,\n",
    "    x=\"bin_str\",\n",
    "    y=\"average_rating\",\n",
    "    title=\"Average Recipe Rating by Calories (Uniform 500 Cal Bins)\",\n",
    "    labels={\"bin_str\": \"Calories Range\", \"average_rating\": \"Average Rating\"}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis={\n",
    "        \"categoryorder\":\"array\",\n",
    "        \"categoryarray\": agg_cal[\"bin_str\"].tolist()\n",
    "    }\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMAR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id  calories  total fat  sugar  ...  protein  saturated fat  \\\n",
      "19     275070      19.1        0.0   15.0  ...      1.0            0.0   \n",
      "47     275139    3960.5      502.0  873.0  ...     98.0          140.0   \n",
      "123    275291     277.5        7.0   18.0  ...     11.0            4.0   \n",
      "...       ...       ...        ...    ...  ...      ...            ...   \n",
      "83775  537429     333.2       11.0   25.0  ...     10.0            5.0   \n",
      "83779  537543    1617.0      104.0  213.0  ...     40.0          203.0   \n",
      "83780  537671     207.9       12.0   93.0  ...      6.0            8.0   \n",
      "\n",
      "       carbohydrates  average_rating  \n",
      "19               1.0             NaN  \n",
      "47              89.0             NaN  \n",
      "123             17.0             NaN  \n",
      "...              ...             ...  \n",
      "83775           15.0             NaN  \n",
      "83779           80.0             NaN  \n",
      "83780           10.0             NaN  \n",
      "\n",
      "[2609 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Isolating NaN value rows\n",
    "nan_rows = df[df.isna().any(axis=1)]\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, all missing values occur in the average_rating column. Any 0s in this column with NaN values, under the assumption that a rating of 0 did not represent a real rating but rather a missing or unreported value. This means that the missingness in the average_rating column is directly related to the value itself—specifically, the missingness occurs when the original value was 0.\n",
    "\n",
    "Since the probability of a value being missing is determined by its value, the missingness mechanism is Not Missing At Random (NMAR). We cannot determine this just by analyzing the observed data; instead, it requires understanding the data-generating process and recognizing that missing ratings are systematically related to their values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name             1\n",
      "description    114\n",
      "user_id          1\n",
      "recipe_id        1\n",
      "date             1\n",
      "rating           1\n",
      "review          58\n",
      "dtype: int64\n",
      "For Ratings: Observed diff: -0.02, p-value: 0.0000\n",
      "For Minutes: Observed diff: 33.56, p-value: 0.6610\n"
     ]
    }
   ],
   "source": [
    "# Find columns with missing values\n",
    "print(merged_df.isna().sum()[merged_df.isna().sum() > 0])\n",
    "\n",
    "# Create an indicator for rating present\n",
    "merged_df['has_rating'] = merged_df['rating'].notna().astype(int)\n",
    "\n",
    "# Create missingness indicator for reviews\n",
    "merged_df['missing_review'] = merged_df['review'].isna()\n",
    "\n",
    "# Permutation test function\n",
    "def permutation_test(df, test_col, missing_indicator, n_permutations=1000):\n",
    "    temp_df = df.dropna(subset=[test_col])\n",
    "    one = temp_df[temp_df[missing_indicator]][test_col]\n",
    "    zero = temp_df[~temp_df[missing_indicator]][test_col]\n",
    "    observed_diff = one.mean() - zero.mean()\n",
    "    differences = []\n",
    "    for i in range(n_permutations):\n",
    "        shuffled = np.random.permutation(temp_df[missing_indicator])\n",
    "        difference = temp_df[test_col][shuffled].mean() - temp_df[test_col][~shuffled].mean()\n",
    "        differences.append(difference)\n",
    "    p_value = np.mean(np.abs(differences) >= np.abs(observed_diff))\n",
    "    return observed_diff, p_value\n",
    "\n",
    "# Run  permutation test for has_rating\n",
    "obs_diff_rating, p_val_rating = permutation_test(merged_df, 'has_rating', 'missing_review')\n",
    "print(f\"For Ratings: Observed diff: {obs_diff_rating:.2f}, p-value: {p_val_rating:.4f}\")\n",
    "\n",
    "# Run permutation test for minutes\n",
    "obs_diff_minutes, p_val_minutes = permutation_test(merged_df, 'minutes', 'missing_review')\n",
    "print(f'For Minutes: Observed diff: {obs_diff_minutes:.2f}, p-value: {p_val_minutes:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation tests were conducted to analyze the dependency of missingness in the review column on both the presence of a rating (has_rating) and the preparation time (minutes). The observed difference in the probability of having a rating between rows with and without missing reviews was -0.02, with a p-value of 0.0000, indicating a statistically significant association. In contrast, the observed difference in the average preparation time was 33.56 minutes, with a p-value of 0.6610, suggesting no statistically significant association. These results provide evidence that the missingness in the review column is related to the presence of a rating, but not to the preparation time of the recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis (H₀): The average rating of low-calorie recipes is equal to the average rating of high-calorie recipes.\n",
    "\n",
    "Alternative hypothesis (H₁): The average rating of low-calorie recipes is not equal to the average rating of high-calorie recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference: -0.024569589951149773\n",
      "p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Split the data into two groups based on calorie level and extract the average ratings\n",
    "low_cal = df[df['calories'] <= 500]['average_rating'].dropna()\n",
    "high_cal = df[df['calories'] > 500]['average_rating'].dropna()\n",
    "\n",
    "# Compute the observed difference in average rating between high- and low-calorie recipes\n",
    "observed_diff = high_cal.mean() - low_cal.mean()\n",
    "print(\"Observed difference:\", observed_diff)\n",
    "\n",
    "# Combine the two groups into one dataset\n",
    "combined = pd.concat([low_cal, high_cal])\n",
    "labels = ['low'] * len(low_cal) + ['high'] * len(high_cal)\n",
    "\n",
    "n_reps = 1000\n",
    "diffs = []\n",
    "\n",
    "# Run permutation test\n",
    "for _ in range(n_reps):\n",
    "    shuffled = np.random.permutation(labels)\n",
    "    group1 = combined[np.array(shuffled) == 'low']\n",
    "    group2 = combined[np.array(shuffled) == 'high']\n",
    "    diffs.append(group2.mean() - group1.mean())\n",
    "\n",
    "# Compute the p-value: proportion of permuted diffs as or more extreme than the observed\n",
    "p_val = np.mean(np.abs(diffs) >= np.abs(observed_diff))\n",
    "print(\"p-value:\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "source": [
    "Prediction: Can we predict the average rating of a recipe based on its nutritional components? For the purposes of this project, we will use calories and sugar.\n",
    "\n",
    "Justification: This is a realistic prediction task because nutrition facts are known at the time a recipe is published, but user ratings are not. Predicting ratings could help surface highly rated recipes early on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Mean Squared Error: 0.6014\n",
      "Intercept: 4.501627908113615\n",
      "Coefficients: [-0. -0.]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "df_clean = df[['calories', 'sugar', 'average_rating']].dropna()\n",
    "\n",
    "X = df_clean[['calories', 'sugar']]\n",
    "y = df_clean['average_rating']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the baseline model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict ratings on the test set\n",
    "y_pred = baseline_model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error as a baseline metric\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Baseline Model Mean Squared Error: {mse:.4f}')\n",
    "\n",
    "# (Optional) Print coefficients\n",
    "print('Intercept:', baseline_model.intercept_)\n",
    "print('Coefficients:', baseline_model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Mean Squared Error: 0.6016\n",
      "Best alpha (ridge penalty): 100\n",
      "Intercept: 4.4938753402568965\n",
      "Coefficients: [ 0.   -0.   -0.   -0.01]\n"
     ]
    }
   ],
   "source": [
    "# Add engineered features to the train and test sets\n",
    "X_train_f = X_train.copy()\n",
    "X_test_f = X_test.copy()\n",
    "\n",
    "# Feature 1: Interaction between calories and sugar\n",
    "X_train_f['calories_x_sugar'] = X_train_f['calories'] * X_train_f['sugar']\n",
    "X_test_f['calories_x_sugar'] = X_test_f['calories'] * X_test_f['sugar']\n",
    "\n",
    "# Feature 2: Log-transformed calories\n",
    "X_train_f['log_calories'] = np.log1p(X_train_f['calories'])\n",
    "X_test_f['log_calories'] = np.log1p(X_test_f['calories'])\n",
    "\n",
    "# Fit a Ridge Regression by hand (using numpy) and grid search alpha\n",
    "feature_cols = ['calories', 'sugar', 'calories_x_sugar', 'log_calories']\n",
    "\n",
    "# Standardize features based on training data\n",
    "means = X_train_f[feature_cols].mean()\n",
    "stds = X_train_f[feature_cols].std()\n",
    "X_train_scaled = (X_train_f[feature_cols] - means) / stds\n",
    "X_test_scaled = (X_test_f[feature_cols] - means) / stds\n",
    "\n",
    "# Manual grid search for Ridge penalty\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "best_alpha = None\n",
    "best_mse = np.inf\n",
    "best_coefs = None\n",
    "best_intercept = None\n",
    "\n",
    "for i in alphas:\n",
    "    # Closed-form Ridge regression: w = (X^T X + alpha*I)^-1 X^T y\n",
    "    X_mat = np.hstack([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled.values])\n",
    "    I = np.eye(X_mat.shape[1])\n",
    "    I[0, 0] = 0  # Don't regularize intercept\n",
    "    w = np.linalg.inv(X_mat.T @ X_mat + i * I) @ (X_mat.T @ y_train.values)\n",
    "    X_test_mat = np.hstack([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled.values])\n",
    "    y_pred_final = X_test_mat @ w\n",
    "    mse = ((y_pred_final - y_test.values) ** 2).mean()\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_alpha = i\n",
    "        best_coefs = w[1:]\n",
    "        best_intercept = w[0]\n",
    "\n",
    "print(f'Final Model Mean Squared Error: {best_mse:.4f}')\n",
    "print(f'Best alpha (ridge penalty): {best_alpha}')\n",
    "print('Intercept:', best_intercept)\n",
    "print('Coefficients:', best_coefs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
